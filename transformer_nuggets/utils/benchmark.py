import logging
import random
from contextlib import contextmanager, nullcontext
from dataclasses import dataclass, field
from pathlib import Path
from collections.abc import Callable

import torch
import torch.utils.benchmark as benchmark
from torch._inductor.utils import do_bench_using_profiling

from torch.cuda._memory_viz import profile_plot
from torch.profiler import profile, ProfilerActivity, record_function, schedule

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())


class bcolors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


@dataclass
class ProfileConfig:
    file_path: str | None = None
    name: str | None = None
    cuda: bool = True
    iters: int = 0
    warmup_iters: int = 0
    sync: bool = False
    extra_kwargs: dict = field(default_factory=dict)
    memory_profile_path: str | None = None
    row_limit: int = 10


def benchmark_torch_function_in_microseconds(func: Callable, *args, **kwargs) -> float:
    # warmup
    for _ in range(5):
        func(*args, **kwargs)
    t0 = benchmark.Timer(
        stmt="func(*args, **kwargs)",
        globals={"args": args, "kwargs": kwargs, "func": func},
    )
    return t0.adaptive_autorange(min_run_time=0.1).median * 1e6


def benchmark_cuda_function_in_microseconds(func: Callable, *args, **kwargs) -> float:
    """Thin wrapper around do_bench_using_profiling"""
    no_args = lambda: func(*args, **kwargs)
    time = do_bench_using_profiling(no_args)
    return time * 1e3


def profile_function(
    config: ProfileConfig, func: Callable, *args, **kwargs
) -> torch.profiler.profile:
    """Profile a torch function and save the result to a file"""
    seed = 123
    random.seed(seed)
    torch.manual_seed(seed)

    activities = [ProfilerActivity.CPU]
    if config.cuda:
        activities.append(ProfilerActivity.CUDA)

    if config.warmup_iters >= 0:
        for _ in range(config.warmup_iters):
            func(*args, **kwargs)
    if config.sync:
        torch.cuda.synchronize()
    name_context = nullcontext() if config.name is None else record_function(config.name)
    profile_memory = config.memory_profile_path is not None
    with profile(
        activities=activities,
        profile_memory=profile_memory,
        record_shapes=profile_memory,
        with_stack=profile_memory,
        **config.extra_kwargs,
    ) as prof:
        for _ in range(config.iters):
            with name_context:
                func(*args, **kwargs)
                if config.sync:
                    torch.cuda.synchronize()

    if config.file_path is not None:
        trace_path = Path(config.file_path).with_suffix(".json")
        prof.export_chrome_trace(str(trace_path))
        logger.info(f"ðŸ’¾ Trace file ðŸ“„ saved to: {bcolors.OKGREEN}{trace_path}{bcolors.ENDC}")

    if profile_memory:
        with open(config.memory_profile_path, "w") as f:
            f.write(profile_plot(prof))

    if config.file_path is None:
        sort_by = "cpu_time_total" if not config.cuda else "cuda_time_total"
        print(prof.key_averages().table(sort_by=sort_by, row_limit=config.row_limit))

    return prof


@contextmanager
def max_memory_usage(log: bool = False, precision: int = 2) -> int:
    """Prints the maximum CUDA memory usage at the end of a context manager

    Args:
        log (bool): Whether to print the memory usage to the console
        precision (int): The number of decimal places to print

    Usage:
    ```
        with print_max_memory_usage():
            # code to profile
    ```
    Returns:
        max_memory (int): The maximum CUDA memory usage in GiB
    """
    try:
        yield
    finally:
        max_memory = torch.cuda.max_memory_allocated()
        if log:
            max_memory_gib = max_memory / (1024**3)
            print(f"Max CUDA Memory Allocated: {max_memory_gib:.{precision}f} GiB")
    return max_memory


class cuda_memory_usage:
    """Prints the difference CUDA memory usage at the end of a context manager

    Args:
        log (bool): Whether to print the memory usage to the console
        precision (int): The number of decimal places to print

    Usage:
    ```
        with cuda_memory_usage() as mem:
            # code to profile
        print(mem.memory_usage)
    ```

    """

    def __init__(self, log=False, precision=2):
        self.log = log
        self.precision = precision
        self.memory_usage = 0

    def __enter__(self):
        self.initial_memory = torch.cuda.memory_allocated()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.memory_usage = torch.cuda.memory_allocated() - self.initial_memory
        if self.log:
            memory_usage_gib = self.memory_usage / (1024**3)
            print(f"CUDA memory usage: {memory_usage_gib:.{self.precision}f} GiB")


@contextmanager
def save_memory_snapshot(file_path: Path):
    """Save a memory snapshot information to a folder

    Args:
        file_path: The path to the folder to save the snapshot to
                    will create the folder if it doesn't exist

    Usage:
    ```
        with save_memory_snapshot(file_path):
            # code to profile
    ```
    """
    from transformer_nuggets import init_logging

    init_logging()
    try:
        import torch.distributed as dist

        dist_avail = True
    except ImportError:
        pass

    dist_avail = dist_avail and dist.is_initialized()
    if dist_avail:
        if not file_path.is_dir():
            raise ValueError(
                f"{file_path} is not a directory, but is required for distributed profiling"
            )
    else:
        if file_path.is_dir():
            raise ValueError(f"{file_path} is a directory")

    # make parent dir
    file_path.parent.mkdir(parents=True, exist_ok=True)
    torch.cuda.memory._record_memory_history()
    try:
        yield
    finally:
        s = torch.cuda.memory._snapshot()
        if dist_avail:
            local_rank = dist.get_rank()
            output_path = file_path / f"_rank_{local_rank}.html"
        else:
            output_path = file_path.with_suffix(".html")
        with open(output_path, "w") as f:
            f.write(torch.cuda._memory_viz.trace_plot(s))
            logger.info(f"ðŸ’¾ Trace file ðŸ“„ saved to: {bcolors.OKGREEN}{output_path}{bcolors.ENDC}")


def _is_distributed():
    try:
        import torch.distributed as dist

        return dist.is_initialized()
    except ImportError:
        pass
    return False


def attach_oom_observer(save_path: Path | None = None, max_entries: int = 1000000):
    """
    Attach an out-of-memory (OOM) observer to the CUDA device.
    The observer will save a memory snapshot when an OOM error occurs.

    Args:
        save_path (Path): Directory where memory snapshots will be saved.
                         The cwd will be used.
        max_entries (int): Maximum number of memory history entries to record.
                           Default is 1000000.

    Usage:
    ```
        attach_oom_observer(Path("memory_snapshots"))
        # All cuda cuda events from this point to OOM program termination will be recorded and saved
        <Code that OOMS>
    ```
    """
    import torch.cuda.memory

    if save_path is None:
        save_path = Path.cwd() / "memory_snapshots"
    trace_dir = save_path
    trace_dir.mkdir(parents=True, exist_ok=True)
    assert trace_dir.is_dir(), "save_path must be a directory."

    def oom_observer(device, alloc, device_alloc, device_free):
        try:
            rank = "0"
            if _is_distributed():
                import torch.distributed as dist

                rank = dist.get_rank()

            curr_trace_name = f"memory_snapshots_rank_{rank}_snapshot.html"
            current_trace_name = trace_dir / Path(curr_trace_name)

            logging.info("Saving allocated state during OOM")
            snapshot = torch.cuda.memory._snapshot()
            with open(current_trace_name, "w") as f:
                f.write(torch.cuda._memory_viz.trace_plot(snapshot))
            logging.info(f"Wrote memory snapshot to {current_trace_name}")
        except Exception as e:
            logging.error(f"Failed to save memory snapshot: {e}")

    torch._C._cuda_attach_out_of_memory_observer(oom_observer)
    torch.cuda.memory._record_memory_history(max_entries=max_entries)


def get_process_rank():
    """Get process rank even if distributed is not initialized"""
    import os

    # Check for LOCAL_RANK which torchrun sets
    if "LOCAL_RANK" in os.environ:
        return int(os.environ["LOCAL_RANK"])
    return None


@contextmanager
def profiler(
    path: Path | str,
    record_shapes: bool = True,
    profile_memory: bool = False,
    with_stack: bool = False,
    warmup: int = 0,
):
    """Thin wrapper around torch.profiler

    Args:
        path: The path to save the trace file to
        record_shapes: Record shapes of tensors
        profile_memory: Profile memory usage
        with_stack: Record stack traces - Blows up memory
        warmup: If greater than 0 then it will warmup record before recording

    Usage:
    ```
        with profiler(Path("trace.json")):
            # code to profile

        # With steps (e.g. in a training loop) this will record 7 iterations
        with profiler(Path("trace.json"), warmup=3) as p:
            for i in range(10):
                # Your code for this step (e.g. forward, backward, optimize)

                # Call step() after each iteration
                p.step()
    ```
    """
    from transformer_nuggets import init_logging

    init_logging()

    if not isinstance(path, Path):
        path = Path(path)

    rank = get_process_rank()

    # Create path with suffix
    path = path.with_suffix(".json")

    # Add rank to filename if distributed
    if rank is not None:
        path = path.parent / f"{path.stem}_rank_{rank}{path.suffix}"

    # make parent dir if it doesn't exist
    output_dir = path.parent
    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"ðŸ’¾ Trace file ðŸ“„ saved to: {bcolors.OKGREEN}{path}{bcolors.ENDC}")

    def trace_handler(prof) -> None:
        prof.export_chrome_trace(path.as_posix())

    prof_sched = schedule(wait=0, warmup=warmup, active=int(1_000_000)) if warmup > 0 else None
    profiler = torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        on_trace_ready=trace_handler,
        record_shapes=record_shapes,
        profile_memory=profile_memory,
        with_stack=with_stack,
        schedule=prof_sched,
    )

    try:
        profiler.start()
        yield profiler
    finally:
        profiler.stop()
